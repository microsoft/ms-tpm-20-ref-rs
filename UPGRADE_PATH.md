# Notes on upgrading `microsoft/ms-tpm-20-ref`

- See [Migrating between `ms-tpm-20-ref` versions](#migrating-between-ms-tpm-20-ref-versions) for a brief discussion on migrating between TPM library versions. Make sure to read this first!
- See [Bumping the `ms-tpm-20-ref` submodule](#bumping-the-ms-tpm-20-ref-submodule) for an overview of things to keep in mind when bumping the underlying TPM library version.

**Please update this document if more progress has been made wrt. the TPM migration story.**

## Migrating between `ms-tpm-20-ref` versions

So, you want to switch to a newer version of `microsoft/ms-tpm-20-ref`, while still maintaining backwards compatibility with persistent state generated by earlier library revisions?

Unfortunately, **this is currently impossible.**

Until the underlying `ms-tpm-20-ref` library is adds some kind of built-in nvmem migration functionality, it would be (nearly) impossible to migrate nvmem state across `ms-tpm-20-ref` revisions.

### Case study: Migrating from 1.38 to 1.62

In July/August 2021, we (Daniel Prilik, David Altobelli) explored the potential up migrating data from a 1.38 TPM to a 1.62 TPM, with the goal of migrating the TPM nvmem blob from a traditional Hyper-V VM to a HvLite / Underhill VM.

From this point, I'll be writing from my own perspective, so when I say "I", I mean "Daniel Prilik".

David and I made it quite far, and we even managed to get to the point where the EK/SRK of the vTPM stayed the same post-migration, but as we explored further, we realized that while it would be _technically_ possible to perform a "perfect" 1.38 to 1.62 migration, it would require some non-trivial data munging + deep knowledge of the `ms-tpm-2o-ref` codebase.

What follows is a brief overview of the steps we took while exploring the feasibility of a 1.38 to 1.62 migration:

#### 1. Swapping out the `microsoft/ms-tpm-20-ref` submodule version to `v1.62`

This entailed going through the checklist outlined in [Bumping the `ms-tpm-20-ref` submodule](#bumping-the-ms-tpm-20-ref-submodule).

#### 2. Matching build-time `#define`s

The specific structure and layout of the nvmem blob is dependant on a wide range of compile-time options exposed by the TPM library, such as supported crypto algorithms, TPM commands, algorithm hash sizes, configurable buffer sizes, etc...

This is a daunting task, and the only reason I was able to pull this off with any reasonable degree of confidence was thanks to David's foresight to include some "fingerprinting" code in the legacy 1.38 implementation, which gave me a way to check if many of the "important" TPM options (see list above) were lining up correctly.

If it wasn't for that fingerprinting code, it would have been a _lot_ harder to sort through the hundreds (if not thousands) of `#define`s to figure out which ones "mattered" (assuming you weren't intimately familiar with the TPM library's internal structure).

#### 3. Matching the broad strokes of the nvmem blob's structure

With the build-time `#define`s ostensibly matching between the 1.38 and 1.62 implementations, I compared the layout of the resulting nvmem blobs by intrusively adding some debug-prints / asserts into the TPM library to dump the offsets of overarching nvmem structures:

```cpp
void NvManufacture(void)
{
    printf("%lx\n", NV_PERSISTENT_DATA);
    printf("%lx\n", NV_STATE_RESET_DATA);
    printf("%lx\n", NV_STATE_CLEAR_DATA);
    printf("%lx\n", NV_ORDERLY_DATA);
    printf("%lx\n", NV_INDEX_RAM_DATA);
    printf("%lx\n", NV_USER_DYNAMIC);

    // ...
}
```

These constants correspond to the handful of broad structures that correspond to the final nvmem blob file. If we wanted to migrate data between 1.38 and 1.62, we would have to match these offsets as close as possible, and if that wasn't possible, perform some-kind of data-munching to get things to line up.

At first blush, it seemed that the structures were _somewhat_ close, but unfortunately, things weren't lining up perfectly, and there seemed to be some new data added to the nvmem blob at some point between 1.38 and 1.62.

By manually diffing of the nvmem structs between the 1.38 and 1.62 codebases, I noticed was that the `ORDERLY_DATA` struct had some new fields in 1.62 that weren't present in 1.38:

```cpp
typedef struct orderly_data
{
    // ...
#if ACCUMULATE_SELF_HEAL_TIMER
    UINT64              selfHealTimer;  // current value of s_selfHealTimer
    UINT64              lockoutTimer;   // current value of s_lockoutTimer
    UINT64              time;           // current value of g_time at shutdown
#endif // ACCUMULATE_SELF_HEAL_TIMER

    //          new fields            //
    // \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/ //

// These are the ACT Timeout values. They are saved with the other timers
#define DefineActData(N)  ACT_STATE      ACT_##N;
    FOR_EACH_ACT(DefineActData)

 // this is the 'signaled' attribute data for all the ACT. It is done this way so
 // that they can be manipulated by ACT number rather than having to access a
 // structure.
    UINT16              signaledACT;
    UINT16              preservedSignaled;
} ORDERLY_DATA;
```

1.38 doesn't have any notion of ACTs, so when they were added to the library, they also added new fields to the nvmem blob!

To account for this new data, I updated our ad-hoc 1.38 to 1.62 nvmem migration routine (written in Rust) to insert some dummy-ACT "padding" bytes when migrating 1.38 state.

At this point, in what seemed to be an absolutely _miraculous_ event at the time, the 1.38 fingerprints and 1.62 fingerprints lined up!

...unfortunately, when booting a 1.62 VM using migrated 1.32 state, things were _still not working_.

#### 4. Discovering non-fingerprinted differences

At this point, things started to get _really_ nasty, and we entered the territory of manually inspecting hex-dumps, staring at obtuse TPM library code, and doing side-by-side debugging of in-memory state.

Though a combination of the above techniques, and quite a bit of luck, I noticed that [this commit](https://github.com/microsoft/ms-tpm-20-ref/commit/5847c02ff793114343dc18e92e60e2919fadc0b8)changed the `OBJECT` struct, which gets serialized as part of the `NV_USER_DYNAMIC` section of the nvmem blob:

```cpp
typedef struct OBJECT
{
    // The attributes field is required to be first followed by the publicArea.
    // This allows the overlay of the object structure and a sequence structure
    OBJECT_ATTRIBUTES   attributes;         // object attributes
    TPMT_PUBLIC         publicArea;         // public area of an object
    TPMT_SENSITIVE      sensitive;          // sensitive area of an object

    // this `privateExponent` exists in 1.38, but not in 1.62!! //
    // \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/ //

#ifdef  TPM_ALG_RSA
    privateExponent_t   privateExponent;    // Additional field for the private
#endif

    TPM2B_NAME          qualifiedName;      // object qualified name
    TPMI_DH_OBJECT      evictHandle;        // if the object is an evict object,
                                            // the original handle is kept here.
                                            // The 'working' handle will be the
                                            // handle of an object slot.

    TPM2B_NAME          name;               // Name of the object name. Kept here
                                            // to avoid repeatedly computing it.
} OBJECT;
```

Because of this change, the TPM library was running into runtime errors when loading 1.38 nvdynamic data, since the structures had changed in size, and the fields had bogus data!

As a smoke test, I changed the defn of the `OBJECT` struct in 1.62 to include an extra `char padding [size_of(privateExponent_t)]` field, and upon doing do, we were able to successfully boot into a Windows VM with the migrated TPM state! The EK/SRK even matched!

Unfortunately, the solution of "just insert some padding bytes" was neither a long-term nor short-term correct, as it effectively discarded the data that was encoded in the `privateExponent` field.

Reading through the aforementioned commit, it seems that the `privateExponent` field was removed, and instead, the data it contained was re-packaged into the `sensitive` field of the struct. As such, to "properly" migrate this data over would have required my data migration script to:

1. discover all serialized instances of `OBJECT` in the nvmem's NV_DYNAMIC data section
2. figure out how to convert the `privateExponent_t` representation of the data into the `TPMT_SENSITIVE` representation
3. "cut" the `privateExponent` field from the 1.38 nvmem blob to keep everything lined-up

Now, up until now, David and I held on to the hope that the migration _wouldn't_ require this kind of intrusive data munging, and we could simply throw in some padding bytes / shuffle some parts of the nvmem blob around to get everything to line up. Unfortunately, as this particular example showed us, this wasn't guaranteed to be the case.

#### 5. Throwing in the towel

While the aforementioned `privateExponent` data munging wasn't _impossible_ to pull off, and we probably could have gotten it working with a few more days of effort, it hinted at a broader issue we would have to consider as we continued to work on the migration work:

_Theoretically_, we could keep poking and prodding at the TPM library to try and find more subtle incompatibilities, writing ad-hoc migration routines as things come up. e.g: this particular issue was related to RSA keys, but what about other key types? Could there be other subtle issues that only crop-up at runtime when using a specific subset of TPM functionality?

_Realistically_, getting an air-tight compat story would require an unbounded amount of engineering effort on our end, and we'd be unlikely to ever have 100% confidence that we didn't miss anything.

As such, we decided to throw in the towel, and switched gears to porting over the existing 1.38 TPM code, thereby sidestepping any/all compatibility issues we might face.

### Conclusion

The case study above should provide a good glimpse into the effort required to migrate between TPM versions. i.e: it's _possible_, but it requires a _lot_ of engineering effort, and there's not good way to _statically_ assert that the migration is 100% correct.

We have filed a formal request with the maintainers of the `microsoft/ms-tpm-20-ref` library to provide some kind of migration strategy for future TPM versions, though there is no firm timeline on when that'd be available. Moreover, even if they add a built-in migration utility, it's unlikely that they would backport it to earlier versions.

## Bumping the `ms-tpm-20-ref` submodule

Since this crate also provides an _implementation_ of the platform API, any changes in the underlying `ms-tpm-20-ref` platform API will require updating this crate's implementation as well. This process cannot be automated, and will require a human to audit / validate that all platform API FFI signatures and implementations are unchanged between versions - updating them as necessary if differences crop up.
